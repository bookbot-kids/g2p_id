{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#g2p-id-indonesian-grapheme-to-phoneme-converter","title":"g2p ID: Indonesian Grapheme-to-Phoneme Converter","text":"<p>This library is developed to convert Indonesian (Bahasa Indonesia) graphemes (words) to phonemes in IPA. We followed the methods and designs used in the English equivalent library, g2p.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install g2p_id_py\n</code></pre>"},{"location":"#how-to-use","title":"How to Use","text":"<pre><code>from g2p_id import G2p\n\ntexts = [\n    \"Apel itu berwarna merah.\",\n    \"Rahel bersekolah di Jakarta.\",\n    \"Mereka sedang bermain bola di lapangan.\",\n]\n\ng2p = G2p()\nfor text in texts:\n    print(g2p(text))\n\n&gt;&gt; [['a', 'p', '\u0259', 'l'], ['i', 't', 'u'], ['b', '\u0259', 'r', 'w', 'a', 'r', 'n', 'a'], ['m', 'e', 'r', 'a', 'h'], ['.']]\n&gt;&gt; [['r', 'a', 'h', 'e', 'l'], ['b', '\u0259', 'r', 's', '\u0259', 'k', 'o', 'l', 'a', 'h'], ['d', 'i'], ['d\u0292', 'a', 'k', 'a', 'r', 't', 'a'], ['.']]\n&gt;&gt; [['m', '\u0259', 'r', 'e', 'k', 'a'], ['s', '\u0259', 'd', 'a', '\u014b'], ['b', '\u0259', 'r', 'm', 'a', 'i', 'n'], ['b', 'o', 'l', 'a'], ['d', 'i'], ['l', 'a', 'p', 'a', '\u014b', 'a', 'n'], ['.']]\n</code></pre>"},{"location":"#references","title":"References","text":"<pre><code>@misc{g2pE2019,\n  author = {Park, Kyubyong &amp; Kim, Jongseok},\n  title = {g2pE},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/Kyubyong/g2p}}\n}\n</code></pre> <pre><code>@misc{TextProcessor2021,\n  author = {Cahya Wirawan},\n  title = {Text Processor},\n  year = {2021},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/cahya-wirawan/text_processor}}\n}\n</code></pre>"},{"location":"#contributors","title":"Contributors","text":""},{"location":"algorithm/","title":"Algorithm","text":"<p>This is heavily inspired from the English g2p.</p> <ol> <li>Spells out arabic numbers and some currency symbols, e.g. <code>Rp 200,000 -&gt; dua ratus ribu rupiah</code>. This is borrowed from Cahya's code.</li> <li>Attempts to retrieve the correct pronunciation for homographs based on their POS (part-of-speech) tags.</li> <li>Looks up a lexicon (pronunciation dictionary) for non-homographs. This list is originally from ipa-dict, and we later made a modified version.</li> <li>For OOVs, we predict their pronunciations using either a BERT model or an LSTM model.</li> </ol>"},{"location":"algorithm/#phoneme-and-grapheme-sets","title":"Phoneme and Grapheme Sets","text":"<pre><code>graphemes = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\nphonemes = ['a', 'b', 'd', 'e', 'f', '\u0261', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'z', '\u014b', '\u0259', '\u0272', 't\u0283', '\u0283', 'd\u0292', 'x', '\u0294']\n</code></pre>"},{"location":"algorithm/#homographs","title":"Homographs","text":"<p>Indonesian words (as far as we know) only have one case of homograph, that is, differing ways to pronounce the letter <code>e</code>. For instance, in the word <code>apel</code> (meaning: apple), the letter <code>e</code> is a mid central vowel <code>\u0259</code>. On the other hand, the letter <code>e</code> in the word <code>apel</code> (meaning: going to a significant other's house; courting), is a closed-mid front unrounded vowel <code>e</code>. Sometimes, a word might have &gt;1 <code>e</code>s pronounced in both ways, for instance, <code>mereka</code> (meaning: they) is pronounced as <code>m\u0259reka</code>. Because of this, there needs a way to disambiguate homographs, and in our case, we used their POS (part-of-speech) tags. However, this is not a foolproof method since homographs may even have the same POS tag. We are considering a contextual model to handle this better.</p>"},{"location":"algorithm/#oov-prediction","title":"OOV Prediction","text":"<p>Initially, we relied on a sequence2sequence LSTM model for OOV (out-of-vocabulary) prediction. This was a natural choice given that it can \"automatically\" learn the rules of grapheme-to-phoneme conversion without having to determine the rules by hand. However, we soon noticed that despite its validation results, the model performed poorly on unseen words, especially on longer ones. We needed a more controllable model that makes predictions on necessary characters only. We ended up with a customized BERT that predicts the correct pronunciation of the letter <code>e</code> while keeping the rest of the string unchanged. We then apply a hand-written g2p conversion algorithm that handles the other characters.</p> <p>You can find more detail in this blog post.</p>"},{"location":"algorithm/#pos-tagging","title":"POS Tagging","text":"<p>We trained an NLTK PerceptronTagger on the POSP dataset, which achieved 0.956 and 0.945 F1-score on the valid and test sets, respectively. Given its performance and speed, we decided to adopt this model as the POS tagger for the purpose of disambiguating homographs, which is just like the English g2p library.</p> tag precision recall f1-score B-$$$ 1.000000 1.000000 1.000000 B-ADJ 0.904132 0.864139 0.883683 B-ADK 1.000000 0.986667 0.993289 B-ADV 0.966874 0.976987 0.971904 B-ART 0.988920 0.978082 0.983471 B-CCN 0.997934 0.997934 0.997934 B-CSN 0.986395 0.963455 0.974790 B-INT 1.000000 1.000000 1.000000 B-KUA 0.976744 0.976744 0.976744 B-NEG 0.992857 0.972028 0.982332 B-NNO 0.919917 0.941288 0.930480 B-NNP 0.917685 0.914703 0.916192 B-NUM 0.997358 0.954488 0.975452 B-PAR 1.000000 0.851064 0.919540 B-PPO 0.991206 0.991829 0.991517 B-PRI 1.000000 0.928571 0.962963 B-PRK 0.793103 0.851852 0.821429 B-PRN 0.988327 0.988327 0.988327 B-PRR 0.995465 1.000000 0.997727 B-SYM 0.999662 0.999323 0.999492 B-UNS 0.916667 0.733333 0.814815 B-VBE 1.000000 0.985714 0.992806 B-VBI 0.929119 0.877034 0.902326 B-VBL 1.000000 1.000000 1.000000 B-VBP 0.926606 0.933457 0.930018 B-VBT 0.939759 0.953333 0.946498 --------- --------- -------- -------- macro avg 0.966490 0.946937 0.955913"},{"location":"algorithm/#attempts-that-failed","title":"Attempts that Failed","text":"<ul> <li>Parsed online PDF KBBI, but it turns out that it has very little phoneme descriptions.</li> <li>Scraped online Web KBBI, but it had a daily bandwidth which was too low to be used at this level.</li> </ul>"},{"location":"algorithm/#potential-improvements","title":"Potential Improvements","text":"<p>There is a ton of room for improvements, both from the technical and the linguistic side of the approaches. Consider that a failure of one component may cascade to an incorrect conclusion. For instance, an incorrect POS tag can lead to the wrong phoneme, ditto for incorrect OOV prediction. We propose the following future improvements.</p> <ul> <li> Use a larger pronunciation lexicon instead of having to guess.</li> <li> Find a larger homograph list.</li> <li> Use contextual model instead of character-level RNNs.</li> <li> Consider hand-written rules for g2p conversion.</li> <li> Add to PyPI.</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Hi there! Thanks for taking your time to contribute!</p> <p>We welcome everyone to contribute and we value each contribution, even the smallest ones! We want to make contributing to this project as easy and transparent as possible, whether it's:</p> <ul> <li>Reporting a bug</li> <li>Discussing the current state of the code</li> <li>Submitting a fix</li> <li>Proposing new features</li> <li>Becoming a maintainer</li> </ul>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>Please be mindful to respect our Code of Conduct.</p>"},{"location":"contributing/#we-develop-with-github","title":"We Develop with Github","text":"<p>We use github to host code, to track issues and feature requests, as well as accept pull requests.</p>"},{"location":"contributing/#we-use-github-so-all-code-changes-happen-through-pull-requests","title":"We Use Github, So All Code Changes Happen Through Pull Requests","text":"<p>Pull requests are the best way to propose changes to the codebase. We actively welcome your pull requests:</p> <ol> <li>Fork the repo and create your branch from <code>main</code>.</li> <li>If you've added code that should be tested, add tests.</li> <li>If you've changed APIs, update the documentation.</li> <li>Ensure the test suite passes.</li> <li>Make sure your code lints.</li> <li>Issue that pull request!</li> </ol>"},{"location":"contributing/#any-contributions-you-make-will-be-under-the-apache-20-license","title":"Any contributions you make will be under the Apache 2.0 License","text":"<p>In short, when you submit code changes, your submissions are understood to be under the same Apache 2.0 License that covers the project. Feel free to contact the maintainers if that's a concern.</p>"},{"location":"contributing/#report-bugs-using-githubs-issues","title":"Report bugs using Github's issues","text":"<p>We use GitHub issues to track public bugs. Report a bug by opening a new issue.</p>"},{"location":"contributing/#write-bug-reports-with-detail-background-and-sample-code","title":"Write bug reports with detail, background, and sample code","text":"<p>This is an example of a good and thorough bug report.</p> <p>Great Bug Reports tend to have:</p> <ul> <li>A quick summary and/or background</li> <li>Steps to reproduce</li> <li>Be specific!</li> <li>Give sample code if you can.</li> <li>What you expected would happen</li> <li>What actually happens</li> <li>Notes (possibly including why you think this might be happening, or stuff you tried that didn't work)</li> </ul>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under its Apache 2.0 License.</p>"},{"location":"contributing/#references","title":"References","text":"<p>This document was adapted from the open-source contribution guidelines for Facebook's Draft</p>"},{"location":"reference/bert/","title":"BERT","text":""},{"location":"reference/bert/#g2p_id.bert.BERT","title":"<code> g2p_id.bert.BERT        </code>","text":"<p>Phoneme-level BERT model for predicting the correct phoneme for the letter <code>e</code>. Trained with Keras, and exported to ONNX. ONNX Runtime engine used during inference.</p> Source code in <code>g2p_id/bert.py</code> <pre><code>class BERT:\n    \"\"\"Phoneme-level BERT model for predicting the correct phoneme for the letter `e`.\n    Trained with [Keras](https://keras.io/examples/nlp/masked_language_modeling/),\n    and exported to ONNX. ONNX Runtime engine used during inference.\n    \"\"\"\n\n    def __init__(self):\n        bert_model_path = os.path.join(model_path, \"bert_mlm.onnx\")\n        token2id = os.path.join(model_path, \"token2id.json\")\n        config_path = os.path.join(model_path, \"config.json\")\n        self.model = WrapInferenceSession(bert_model_path, providers=onnxruntime.get_available_providers())\n        with open(config_path, encoding=\"utf-8\") as file:\n            self.config = json.load(file)\n        with open(token2id, encoding=\"utf-8\") as file:\n            self.token2id = json.load(file)\n        self.id2token = {v: k for k, v in self.token2id.items()}\n\n    def predict(self, text: str) -&gt; str:\n        \"\"\"Performs BERT inference, predicting the correct phoneme for the letter `e`.\n\n        Args:\n            text (str): Word to predict from.\n\n        Returns:\n            str: Word after prediction.\n        \"\"\"\n        # `x` is currently OOV, we replace with\n        text = text.replace(\"x\", \"ks\")\n        # mask `e`'s\n        text = \" \".join([c if c != \"e\" else \"[mask]\" for c in text])\n\n        # tokenize and pad to max length\n        tokens = [self.token2id[c] for c in text.split()]\n        padding = [self.token2id[self.config[\"pad_token\"]] for _ in range(self.config[\"max_seq_length\"] - len(tokens))]\n        tokens = tokens + padding\n\n        input_ids = np.array([tokens], dtype=\"int64\")\n        inputs = {\"input_1\": input_ids}\n        prediction = self.model.run(None, inputs)\n\n        # find masked idx token\n        mask_token_id = self.token2id[self.config[\"mask_token\"]]\n        masked_index = np.where(input_ids == mask_token_id)[1]\n\n        # get prediction at masked indices\n        mask_prediction = prediction[0][0][masked_index]\n        predicted_ids = np.argmax(mask_prediction, axis=1)\n\n        # replace mask with predicted token\n        for i, idx in enumerate(masked_index):\n            tokens[idx] = predicted_ids[i]\n\n        return \"\".join([self.id2token[t] for t in tokens if t != 0])\n</code></pre>"},{"location":"reference/bert/#g2p_id.bert.BERT.predict","title":"<code>predict(self, text)</code>","text":"<p>Performs BERT inference, predicting the correct phoneme for the letter <code>e</code>.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Word to predict from.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Word after prediction.</p> Source code in <code>g2p_id/bert.py</code> <pre><code>def predict(self, text: str) -&gt; str:\n    \"\"\"Performs BERT inference, predicting the correct phoneme for the letter `e`.\n\n    Args:\n        text (str): Word to predict from.\n\n    Returns:\n        str: Word after prediction.\n    \"\"\"\n    # `x` is currently OOV, we replace with\n    text = text.replace(\"x\", \"ks\")\n    # mask `e`'s\n    text = \" \".join([c if c != \"e\" else \"[mask]\" for c in text])\n\n    # tokenize and pad to max length\n    tokens = [self.token2id[c] for c in text.split()]\n    padding = [self.token2id[self.config[\"pad_token\"]] for _ in range(self.config[\"max_seq_length\"] - len(tokens))]\n    tokens = tokens + padding\n\n    input_ids = np.array([tokens], dtype=\"int64\")\n    inputs = {\"input_1\": input_ids}\n    prediction = self.model.run(None, inputs)\n\n    # find masked idx token\n    mask_token_id = self.token2id[self.config[\"mask_token\"]]\n    masked_index = np.where(input_ids == mask_token_id)[1]\n\n    # get prediction at masked indices\n    mask_prediction = prediction[0][0][masked_index]\n    predicted_ids = np.argmax(mask_prediction, axis=1)\n\n    # replace mask with predicted token\n    for i, idx in enumerate(masked_index):\n        tokens[idx] = predicted_ids[i]\n\n    return \"\".join([self.id2token[t] for t in tokens if t != 0])\n</code></pre>"},{"location":"reference/bert/#usage","title":"Usage","text":"<pre><code>texts = [\"mengembangkannya\", \"merdeka\", \"pecel\", \"lele\"]\nbert = BERT()\nfor text in texts:\n    print(bert.predict(text))\n</code></pre> <pre><code>&gt;&gt; m\u0259ng\u0259mbangkannya\n&gt;&gt; m\u0259rdeka\n&gt;&gt; p\u0259cel\n&gt;&gt; lele\n</code></pre>"},{"location":"reference/g2p/","title":"G2p","text":""},{"location":"reference/g2p/#g2p_id.g2p.G2p","title":"<code> g2p_id.g2p.G2p        </code>","text":"<p>Grapheme-to-phoneme (g2p) main class for phonemization. This class provides a high-level API for grapheme-to-phoneme conversion.</p> <ol> <li>Preprocess and normalize text</li> <li>Word tokenizes text</li> <li>Predict POS for every word</li> <li>If word is non-alphabetic, add to list (i.e. punctuation)</li> <li>If word is a homograph, check POS and use matching word's phonemes</li> <li>If word is a non-homograph, lookup lexicon</li> <li>Otherwise, predict with a neural network</li> </ol> Source code in <code>g2p_id/g2p.py</code> <pre><code>class G2p:\n    \"\"\"Grapheme-to-phoneme (g2p) main class for phonemization.\n    This class provides a high-level API for grapheme-to-phoneme conversion.\n\n    1. Preprocess and normalize text\n    2. Word tokenizes text\n    3. Predict POS for every word\n    4. If word is non-alphabetic, add to list (i.e. punctuation)\n    5. If word is a homograph, check POS and use matching word's phonemes\n    6. If word is a non-homograph, lookup lexicon\n    7. Otherwise, predict with a neural network\n    \"\"\"\n\n    def __init__(self, model_type=\"BERT\"):\n        \"\"\"Constructor for G2p.\n\n        Args:\n            model_type (str, optional):\n                Type of neural network to use for prediction.\n                Choices are \"LSTM\" or \"BERT\". Defaults to \"BERT\".\n        \"\"\"\n        self.homograph2features = construct_homographs_dictionary()\n        self.lexicon2features = construct_lexicon_dictionary()\n        self.normalizer = TextProcessor()\n        self.tagger = PerceptronTagger(load=False)\n        tagger_path = os.path.join(resources_path, \"id_posp_tagger.pickle\")\n        with open(tagger_path, \"rb\") as f:\n            self.tagger = self.tagger.decode_json_obj(pickle.load(f))\n        self.model: Union[BERT, LSTM] = BERT() if model_type == \"BERT\" else LSTM()\n        self.tokenizer = TweetTokenizer()\n        self.pos_dict = {\n            \"N\": [\"B-NNO\", \"B-NNP\", \"B-PRN\", \"B-PRN\", \"B-PRK\"],\n            \"V\": [\"B-VBI\", \"B-VBT\", \"B-VBP\", \"B-VBL\", \"B-VBE\"],\n            \"A\": [\"B-ADJ\"],\n            \"P\": [\"B-PAR\"],\n        }\n\n    def _preprocess(self, text: str) -&gt; str:\n        \"\"\"Performs preprocessing.\n        (1) Adds spaces in between tokens\n        (2) Normalizes unicode and accents\n        (3) Normalizes numbers\n        (4) Lower case texts\n        (5) Removes unwanted tokens\n\n        Arguments:\n            text (str): Text to preprocess.\n\n        Returns:\n            str: Preprocessed text.\n        \"\"\"\n        text = text.replace(\"-\", \" \")\n        text = re.sub(r\"\\.(?=.*\\.)\", \" \", text)\n        text = \" \".join(self.tokenizer.tokenize(text))\n        text = unicode(text)\n        text = \"\".join(char for char in unicodedata.normalize(\"NFD\", text) if unicodedata.category(char) != \"Mn\")\n        text = self.normalizer.normalize(text).strip()\n        text = text.lower()\n        text = re.sub(r\"[^ a-z'.,?!\\-]\", \"\", text)\n        return text\n\n    def _rule_based_g2p(self, text: str) -&gt; str:\n        \"\"\"Applies rule-based Indonesian grapheme2phoneme conversion.\n\n        Args:\n            text (str): Grapheme text to convert to phoneme.\n\n        Returns:\n            str: Phoneme string.\n        \"\"\"\n        phonetic_mapping = {\n            \"ny\": \"\u0272\",\n            \"ng\": \"\u014b\",\n            \"sy\": \"\u0283\",\n            \"aa\": \"a\u0294a\",\n            \"ii\": \"i\u0294i\",\n            \"oo\": \"o\u0294o\",\n            \"\u0259\u0259\": \"\u0259\u0294\u0259\",\n            \"uu\": \"u\u0294u\",\n            \"'\": \"\u0294\",\n            \"g\": \"\u0261\",\n            \"q\": \"k\",\n            \"j\": \"d\u0292\",\n            \"y\": \"j\",\n            \"x\": \"ks\",\n            \"c\": \"t\u0283\",\n            \"kh\": \"x\",\n        }\n\n        if text.startswith(\"x\"):\n            text = \"s\" + text[1:]\n\n        if text.startswith(\"ps\"):\n            text = text[1:]\n\n        for graph, phone in phonetic_mapping.items():\n            text = text.replace(graph, phone)\n\n        phonemes = [list(phn) if phn not in (\"d\u0292\", \"t\u0283\") else [phn] for phn in re.split(\"(t\u0283|d\u0292)\", text)]\n        return \" \".join([p for phn in phonemes for p in phn])\n\n    def __call__(self, text: str) -&gt; List[List[str]]:\n        \"\"\"Grapheme-to-phoneme converter.\n\n        1. Preprocess and normalize text\n        2. Word tokenizes text\n        3. Predict POS for every word\n        4. If word is non-alphabetic, add to list (i.e. punctuation)\n        5. If word is a homograph, check POS and use matching word's phonemes\n        6. If word is a non-homograph, lookup lexicon\n        7. Otherwise, predict with a neural network\n\n        Args:\n            text (str): Grapheme text to convert to phoneme.\n\n        Returns:\n            List[List[str]]: List of strings in phonemes.\n        \"\"\"\n        text = self._preprocess(text)\n        words = self.tokenizer.tokenize(text)\n        tokens = self.tagger.tag(words)\n\n        prons = []\n        for word, pos in tokens:\n            pron = \"\"\n            if re.search(\"[a-z]\", word) is None:  # non-alphabetic\n                pron = word\n\n            elif word in self.homograph2features:  # check if homograph\n                pron1, pron2, pos1, _ = self.homograph2features[word]\n\n                # check for the matching POS\n                if pos in self.pos_dict[pos1]:\n                    pron = pron1\n                else:\n                    pron = pron2\n\n            elif word in self.lexicon2features:  # non-homographs\n                pron = self.lexicon2features[word]\n\n            else:  # predict for OOV\n                pron = self.model.predict(word)\n                if isinstance(self.model, BERT):\n                    pron = self._rule_based_g2p(pron)\n\n            if pron.endswith(\"\u0294\"):\n                pron = pron[:-1] + \"k\"\n\n            consonants = \"bdjklmnprstw\u0272\"\n            vowels = \"aeiou\u0259\"\n\n            for letter in consonants:\n                pron = pron.replace(f\"\u0294 {letter}\", f\"k {letter}\")\n\n            # add a glottal stop in between consecutive vowels\n            for v1, v2 in permutations(vowels, 2):\n                pron = pron.replace(f\"{v1} {v2}\", f\"{v1} \u0294 {v2}\")\n\n            prons.append(pron.split())\n\n        return prons\n</code></pre>"},{"location":"reference/g2p/#g2p_id.g2p.G2p.__call__","title":"<code>__call__(self, text)</code>  <code>special</code>","text":"<p>Grapheme-to-phoneme converter.</p> <ol> <li>Preprocess and normalize text</li> <li>Word tokenizes text</li> <li>Predict POS for every word</li> <li>If word is non-alphabetic, add to list (i.e. punctuation)</li> <li>If word is a homograph, check POS and use matching word's phonemes</li> <li>If word is a non-homograph, lookup lexicon</li> <li>Otherwise, predict with a neural network</li> </ol> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Grapheme text to convert to phoneme.</p> required <p>Returns:</p> Type Description <code>List[List[str]]</code> <p>List of strings in phonemes.</p> Source code in <code>g2p_id/g2p.py</code> <pre><code>def __call__(self, text: str) -&gt; List[List[str]]:\n    \"\"\"Grapheme-to-phoneme converter.\n\n    1. Preprocess and normalize text\n    2. Word tokenizes text\n    3. Predict POS for every word\n    4. If word is non-alphabetic, add to list (i.e. punctuation)\n    5. If word is a homograph, check POS and use matching word's phonemes\n    6. If word is a non-homograph, lookup lexicon\n    7. Otherwise, predict with a neural network\n\n    Args:\n        text (str): Grapheme text to convert to phoneme.\n\n    Returns:\n        List[List[str]]: List of strings in phonemes.\n    \"\"\"\n    text = self._preprocess(text)\n    words = self.tokenizer.tokenize(text)\n    tokens = self.tagger.tag(words)\n\n    prons = []\n    for word, pos in tokens:\n        pron = \"\"\n        if re.search(\"[a-z]\", word) is None:  # non-alphabetic\n            pron = word\n\n        elif word in self.homograph2features:  # check if homograph\n            pron1, pron2, pos1, _ = self.homograph2features[word]\n\n            # check for the matching POS\n            if pos in self.pos_dict[pos1]:\n                pron = pron1\n            else:\n                pron = pron2\n\n        elif word in self.lexicon2features:  # non-homographs\n            pron = self.lexicon2features[word]\n\n        else:  # predict for OOV\n            pron = self.model.predict(word)\n            if isinstance(self.model, BERT):\n                pron = self._rule_based_g2p(pron)\n\n        if pron.endswith(\"\u0294\"):\n            pron = pron[:-1] + \"k\"\n\n        consonants = \"bdjklmnprstw\u0272\"\n        vowels = \"aeiou\u0259\"\n\n        for letter in consonants:\n            pron = pron.replace(f\"\u0294 {letter}\", f\"k {letter}\")\n\n        # add a glottal stop in between consecutive vowels\n        for v1, v2 in permutations(vowels, 2):\n            pron = pron.replace(f\"{v1} {v2}\", f\"{v1} \u0294 {v2}\")\n\n        prons.append(pron.split())\n\n    return prons\n</code></pre>"},{"location":"reference/g2p/#g2p_id.g2p.G2p.__init__","title":"<code>__init__(self, model_type='BERT')</code>  <code>special</code>","text":"<p>Constructor for G2p.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>Type of neural network to use for prediction. Choices are \"LSTM\" or \"BERT\". Defaults to \"BERT\".</p> <code>'BERT'</code> Source code in <code>g2p_id/g2p.py</code> <pre><code>def __init__(self, model_type=\"BERT\"):\n    \"\"\"Constructor for G2p.\n\n    Args:\n        model_type (str, optional):\n            Type of neural network to use for prediction.\n            Choices are \"LSTM\" or \"BERT\". Defaults to \"BERT\".\n    \"\"\"\n    self.homograph2features = construct_homographs_dictionary()\n    self.lexicon2features = construct_lexicon_dictionary()\n    self.normalizer = TextProcessor()\n    self.tagger = PerceptronTagger(load=False)\n    tagger_path = os.path.join(resources_path, \"id_posp_tagger.pickle\")\n    with open(tagger_path, \"rb\") as f:\n        self.tagger = self.tagger.decode_json_obj(pickle.load(f))\n    self.model: Union[BERT, LSTM] = BERT() if model_type == \"BERT\" else LSTM()\n    self.tokenizer = TweetTokenizer()\n    self.pos_dict = {\n        \"N\": [\"B-NNO\", \"B-NNP\", \"B-PRN\", \"B-PRN\", \"B-PRK\"],\n        \"V\": [\"B-VBI\", \"B-VBT\", \"B-VBP\", \"B-VBL\", \"B-VBE\"],\n        \"A\": [\"B-ADJ\"],\n        \"P\": [\"B-PAR\"],\n    }\n</code></pre>"},{"location":"reference/g2p/#usage","title":"Usage","text":"<pre><code>texts = [\n    \"Apel itu berwarna merah.\",\n    \"Rahel bersekolah di Jakarta.\",\n    \"Mereka sedang bermain bola di lapangan.\",\n]\ng2p = G2p(model_type=\"BERT\")\nfor text in texts:\n    print(g2p(text))\n</code></pre> <pre><code>&gt;&gt; [['a', 'p', '\u0259', 'l'], ['i', 't', 'u'], ['b', '\u0259', 'r', 'w', 'a', 'r', 'n', 'a'], ['m', 'e', 'r', 'a', 'h'], ['.']]\n&gt;&gt; [['r', 'a', 'h', 'e', 'l'], ['b', '\u0259', 'r', 's', '\u0259', 'k', 'o', 'l', 'a', 'h'], ['d', 'i'], ['d\u0292', 'a', 'k', 'a', 'r', 't', 'a'], ['.']]\n&gt;&gt; [['m', '\u0259', 'r', 'e', 'k', 'a'], ['s', '\u0259', 'd', 'a', '\u014b'], ['b', '\u0259', 'r', 'm', 'a', 'i', 'n'], ['b', 'o', 'l', 'a'], ['d', 'i'], ['l', 'a', 'p', 'a', '\u014b', 'a', 'n'], ['.']]\n</code></pre>"},{"location":"reference/lstm/","title":"LSTM","text":""},{"location":"reference/lstm/#g2p_id.lstm.LSTM","title":"<code> g2p_id.lstm.LSTM        </code>","text":"<p>Phoneme-level LSTM model for sequence-to-sequence phonemization. Trained with Keras, and exported to ONNX. ONNX Runtime engine used during inference.</p> Source code in <code>g2p_id/lstm.py</code> <pre><code>class LSTM:\n    \"\"\"Phoneme-level LSTM model for sequence-to-sequence phonemization.\n    Trained with [Keras](https://keras.io/examples/nlp/lstm_seq2seq/),\n    and exported to ONNX. ONNX Runtime engine used during inference.\n    \"\"\"\n\n    def __init__(self):\n        encoder_model_path = os.path.join(model_path, \"encoder_model.onnx\")\n        decoder_model_path = os.path.join(model_path, \"decoder_model.onnx\")\n        g2id_path = os.path.join(model_path, \"g2id.json\")\n        p2id_path = os.path.join(model_path, \"p2id.json\")\n        config_path = os.path.join(model_path, \"config.json\")\n        self.encoder = WrapInferenceSession(\n            encoder_model_path,\n            providers=onnxruntime.get_available_providers(),\n        )\n        self.decoder = WrapInferenceSession(\n            decoder_model_path,\n            providers=onnxruntime.get_available_providers(),\n        )\n        with open(g2id_path, encoding=\"utf-8\") as file:\n            self.g2id = json.load(file)\n        with open(p2id_path, encoding=\"utf-8\") as file:\n            self.p2id = json.load(file)\n        self.id2p = {v: k for k, v in self.p2id.items()}\n        with open(config_path, encoding=\"utf-8\") as file:\n            self.config = json.load(file)\n\n    def predict(self, text: str) -&gt; str:\n        \"\"\"Performs LSTM inference, predicting phonemes of a given word.\n\n        Args:\n            text (str): Word to convert to phonemes.\n\n        Returns:\n            str: Word in phonemes.\n        \"\"\"\n        input_seq = np.zeros(\n            (\n                1,\n                self.config[\"max_encoder_seq_length\"],\n                self.config[\"num_encoder_tokens\"],\n            ),\n            dtype=\"float32\",\n        )\n\n        for idx, char in enumerate(text):\n            input_seq[0, idx, self.g2id[char]] = 1.0\n        input_seq[0, len(text) :, self.g2id[self.config[\"pad_token\"]]] = 1.0\n\n        encoder_inputs = {\"input_1\": input_seq}\n        states_value = self.encoder.run(None, encoder_inputs)\n\n        target_seq = np.zeros((1, 1, self.config[\"num_decoder_tokens\"]), dtype=\"float32\")\n        target_seq[0, 0, self.p2id[self.config[\"bos_token\"]]] = 1.0\n\n        stop_condition = False\n        decoded_sentence = \"\"\n        while not stop_condition:\n            decoder_inputs = {\n                \"input_2\": target_seq,\n                \"input_3\": states_value[0],\n                \"input_4\": states_value[1],\n            }\n            output_tokens, state_memory, state_carry = self.decoder.run(None, decoder_inputs)\n\n            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n            sampled_char = self.id2p[sampled_token_index]\n            decoded_sentence += sampled_char\n\n            if (\n                sampled_char == self.config[\"eos_token\"]\n                or len(decoded_sentence) &gt; self.config[\"max_decoder_seq_length\"]\n            ):\n                stop_condition = True\n\n            target_seq = np.zeros((1, 1, self.config[\"num_decoder_tokens\"]), dtype=\"float32\")\n            target_seq[0, 0, sampled_token_index] = 1.0\n\n            states_value = [state_memory, state_carry]\n\n        return decoded_sentence.replace(self.config[\"eos_token\"], \"\")\n</code></pre>"},{"location":"reference/lstm/#g2p_id.lstm.LSTM.predict","title":"<code>predict(self, text)</code>","text":"<p>Performs LSTM inference, predicting phonemes of a given word.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Word to convert to phonemes.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Word in phonemes.</p> Source code in <code>g2p_id/lstm.py</code> <pre><code>def predict(self, text: str) -&gt; str:\n    \"\"\"Performs LSTM inference, predicting phonemes of a given word.\n\n    Args:\n        text (str): Word to convert to phonemes.\n\n    Returns:\n        str: Word in phonemes.\n    \"\"\"\n    input_seq = np.zeros(\n        (\n            1,\n            self.config[\"max_encoder_seq_length\"],\n            self.config[\"num_encoder_tokens\"],\n        ),\n        dtype=\"float32\",\n    )\n\n    for idx, char in enumerate(text):\n        input_seq[0, idx, self.g2id[char]] = 1.0\n    input_seq[0, len(text) :, self.g2id[self.config[\"pad_token\"]]] = 1.0\n\n    encoder_inputs = {\"input_1\": input_seq}\n    states_value = self.encoder.run(None, encoder_inputs)\n\n    target_seq = np.zeros((1, 1, self.config[\"num_decoder_tokens\"]), dtype=\"float32\")\n    target_seq[0, 0, self.p2id[self.config[\"bos_token\"]]] = 1.0\n\n    stop_condition = False\n    decoded_sentence = \"\"\n    while not stop_condition:\n        decoder_inputs = {\n            \"input_2\": target_seq,\n            \"input_3\": states_value[0],\n            \"input_4\": states_value[1],\n        }\n        output_tokens, state_memory, state_carry = self.decoder.run(None, decoder_inputs)\n\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_char = self.id2p[sampled_token_index]\n        decoded_sentence += sampled_char\n\n        if (\n            sampled_char == self.config[\"eos_token\"]\n            or len(decoded_sentence) &gt; self.config[\"max_decoder_seq_length\"]\n        ):\n            stop_condition = True\n\n        target_seq = np.zeros((1, 1, self.config[\"num_decoder_tokens\"]), dtype=\"float32\")\n        target_seq[0, 0, sampled_token_index] = 1.0\n\n        states_value = [state_memory, state_carry]\n\n    return decoded_sentence.replace(self.config[\"eos_token\"], \"\")\n</code></pre>"},{"location":"reference/lstm/#usage","title":"Usage","text":"<pre><code>texts = [\"mengembangkannya\", \"merdeka\", \"pecel\", \"lele\"]\nlstm = LSTM()\nfor text in texts:\n    print(lstm.predict(text))\n</code></pre> <pre><code>&gt;&gt; m\u0259\u014b\u0259mba\u014bkan\u0272a\n&gt;&gt; m\u0259rdeka\n&gt;&gt; p\u0259t\u0283\u0259l\n&gt;&gt; lele\n</code></pre>"},{"location":"reference/textprocessor/","title":"TextProcessor","text":""},{"location":"reference/textprocessor/#g2p_id.text_processor.TextProcessor","title":"<code> g2p_id.text_processor.TextProcessor        </code>","text":"<p>Indonesian text processor to normalize numerics, currencies, and timezones.</p> Source code in <code>g2p_id/text_processor.py</code> <pre><code>class TextProcessor:\n    \"\"\"Indonesian text processor to normalize numerics, currencies, and timezones.\"\"\"\n\n    def __init__(self):\n        self.measurements = {}\n        self.thousands = [\"ratus\", \"ribu\", \"juta\", \"miliar\", \"milyar\", \"triliun\"]\n        self.months = [\n            \"Januari\",\n            \"Februari\",\n            \"Maret\",\n            \"April\",\n            \"Mei\",\n            \"Juni\",\n            \"Juli\",\n            \"Agustus\",\n            \"September\",\n            \"Oktober\",\n            \"November\",\n            \"Desember\",\n        ]\n        measurements_path = os.path.join(resources_path, \"measurements.tsv\")\n        currencies_path = os.path.join(resources_path, \"currency.tsv\")\n        timezones_path = os.path.join(resources_path, \"timezones.tsv\")\n\n        with open(measurements_path, \"r\", encoding=\"utf-8\") as file:\n            for lines in file:\n                line = lines.strip().split(\"\\t\")\n                self.measurements[line[0]] = line[1]\n\n        self.currencies = {}\n        with open(currencies_path, \"r\", encoding=\"utf-8\") as file:\n            for lines in file:\n                line = lines.strip().split(\"\\t\")\n                self.currencies[line[0]] = line[1]\n\n        self.timezones = {}\n        with open(timezones_path, \"r\", encoding=\"utf-8\") as file:\n            for lines in file:\n                line = lines.strip().split(\"\\t\")\n                self.timezones[line[0]] = line[1]\n\n        self.re_thousands = \"|\".join(self.thousands)\n        self.re_currencies = r\"\\b\" + re.sub(\n            r\"\\|([^|$\u00a3\u20ac\u00a5\u20a9]+)\", r\"|\\\\b\\1\", \"|\".join(list(self.currencies))\n        )\n        self.re_currencies = re.sub(r\"([$\u00a3\u20ac\u00a5\u20a9])\", r\"\\\\\\1\", self.re_currencies)\n        self.re_moneys = (\n            rf\"(({self.re_currencies}) ?([\\d\\.\\,]+)( ({self.re_thousands})?(an)?)?)\"\n        )\n        self.re_measurements = \"|\".join(list(self.measurements))\n        self.re_measurements = rf\"(\\b([\\d\\.\\,]+) ?({self.re_measurements})\\b)\"\n        self.re_timezones = \"|\".join(list(self.timezones))\n        self.re_timezones = (\n            r\"((\\d{1,2})[\\.:](\\d{1,2}) \" + rf\"\\b({self.re_timezones})\\b)\"\n        )\n        self.re_http = re.compile(\n            r\"\"\"\n            (https?://(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.\n            [a-zA-Z0-9()]{1,6}\\b[-a-zA-Z0-9()@:%_\\+.~#?&amp;//=]*)\n            \"\"\",\n            re.X,\n        )\n\n    @staticmethod\n    def is_integer(number: Any) -&gt; bool:\n        \"\"\"Check if integer by type-casting.\n\n        Args:\n            number (Any): Number to check.\n\n        Returns:\n            bool: Is a valid integer.\n        \"\"\"\n        try:\n            int(number)\n            return True\n        except ValueError:\n            return False\n\n    @staticmethod\n    def is_float(number: Any) -&gt; bool:\n        \"\"\"Check if float by type-casting.\n\n        Args:\n            number (Any): Number to check.\n\n        Returns:\n            bool: Is a valid float.\n        \"\"\"\n        try:\n            float(number)\n            return True\n        except ValueError:\n            return False\n\n    def normalize_url(self, text: str) -&gt; str:\n        \"\"\"Removes URL from text.\n\n        Args:\n            text (str): Text with URL to normalize.\n\n        Returns:\n            str: Normalized text with URLs removed.\n        \"\"\"\n        urls = re.findall(self.re_http, text)\n        for url in urls:\n            text = text.replace(url[0], \"\")\n        return text\n\n    def normalize_currency(self, text: str) -&gt; str:\n        \"\"\"Normalizes international and Indonesian (Rupiah) currencies.\n\n        Examples:\n        - `\"$250\"` -&gt; `\"dua ratus lima puluh dollar\"`\n        - `\"Rp 3,000,000\"` -&gt; `\"tiga juta rupiah\"`\n\n        Args:\n            text (str): Text with currency to normalize.\n\n        Returns:\n            str: Normalized text with currency transliterated.\n        \"\"\"\n        moneys = re.findall(self.re_moneys, text)\n        for money in moneys:\n            number: Any = re.sub(\",\", \".\", re.sub(r\"\\.\", \"\", money[2].strip(\" ,.\")))\n            try:\n                if number == \"\":\n                    continue\n                if self.is_integer(number):\n                    number = int(number)\n                elif self.is_float(number):\n                    number = float(number)\n                else:\n                    number = re.sub(r\"[.,]\", \"\", number)\n                    number = int(number)\n                number = num2words(number, to=\"cardinal\", lang=\"id\")\n                text = text.replace(\n                    money[0].strip(\" ,.\"),\n                    f\"{number} {money[3]} {self.currencies[money[1]]}\",\n                )\n            except NotImplementedError as error:\n                print(error)\n                print(f\"Problem with money: &lt;{text}&gt;: {number}\")\n        return text\n\n    def normalize_measurement(self, text: str) -&gt; str:\n        \"\"\"Normalizes measurement units, including its scalar value.\n\n        Examples:\n        - `\"10,5 km\"` -&gt; `\"sepuluh koma lima kilometer\"`\n        - `\"5\u00b0C\"` -&gt; `\"lima derajat celsius\"`\n\n        Args:\n            text (str): Text with measurements to normalize.\n\n        Returns:\n            str: Normalized text with measurements transliterated.\n        \"\"\"\n        units = re.findall(self.re_measurements, text)\n        for unit in units:\n            number: Any = re.sub(\",\", \".\", re.sub(r\"\\.\", \"\", unit[1].strip(\" ,.\")))\n            try:\n                if number == \"\":\n                    continue\n                if re.search(r\"\\.\", number):\n                    number = float(number)\n                else:\n                    number = int(number)\n                number = num2words(number, to=\"cardinal\", lang=\"id\")\n                text = text.replace(\n                    unit[0].strip(\" ,.\"), f\"{number} {self.measurements[unit[2]]}\"\n                )\n            except NotImplementedError as error:\n                print(error)\n                print(f\"Problem with measurements: &lt;{text}&gt;: {number}\")\n        return text\n\n    def normalize_date(self, text: str) -&gt; str:\n        \"\"\"Normalizes dates.\n\n        Examples:\n        - `\"(12/3/2021)\"` -&gt; `\"dua belas Maret dua ribu dua puluh satu\"`\n\n        Args:\n            text (str): Text with dates to normalize.\n\n        Returns:\n            str: Normalized text with dates transliterated.\n        \"\"\"\n        dates = re.findall(r\"(\\((\\d{1,2})/(\\d{1,2})(/(\\d+))?\\))\", text)\n        for date in dates:\n            try:\n                day = num2words(int(date[1]), to=\"cardinal\", lang=\"id\")\n                month: Any = int(date[2]) - 1\n                if month &gt;= 12:\n                    month = 0\n                month = self.months[month]\n                if date[4] != \"\":\n                    year = num2words(int(date[4]), to=\"cardinal\", lang=\"id\")\n                    date_string = f\"{day} {month} {year}\"\n                else:\n                    date_string = f\"{day} {month}\"\n                text = text.replace(date[0], f\" {date_string} \")\n            except NotImplementedError as error:\n                print(error)\n                print(f\"Problem with dates: &lt;{text}&gt;: {date}\")\n        return text\n\n    def normalize_timezone(self, text: str) -&gt; str:\n        \"\"\"Normalizes Indonesian time with timezones.\n\n        Examples:\n        - `\"22.30 WITA\"`\n            -&gt; `\"dua puluh dua lewat tiga puluh menit Waktu Indonesia Tengah\"`\n\n        Args:\n            text (str): Text with timezones to normalize.\n\n        Returns:\n            str: Normalized text with timezones transliterated.\n        \"\"\"\n        timezones = re.findall(self.re_timezones, text)\n        for timezone in timezones:\n            try:\n                hour = num2words(int(timezone[1]), to=\"cardinal\", lang=\"id\")\n                minute = num2words(int(timezone[2]), to=\"cardinal\", lang=\"id\")\n                zone = self.timezones[timezone[3]]\n                if minute == \"nol\":\n                    time_string = f\"{hour} {zone}\"\n                else:\n                    time_string = f\"{hour} lewat {minute} menit {zone}\"\n                text = text.replace(timezone[0], f\"{time_string}\")\n            except NotImplementedError as error:\n                print(error)\n                print(f\"Problem with timezones: &lt;{text}&gt;: {timezone}\")\n        return text\n\n    def normalize_number(self, text: str) -&gt; str:\n        \"\"\"Normalizes Arabic numbers to Indonesian.\n\n        Examples:\n        - `\"1.000\"` -&gt; `\"seribu\"`\n        - `\"10,5\"` -&gt; `\"sepuluh koma lima\"`\n\n        Args:\n            text (str): Text with numbers to normalize.\n\n        Returns:\n            str: Normalized text with numbers transliterated.\n        \"\"\"\n        re_numbers = [r\"([\\d.,]+)\", r\"\\d+\"]\n        for re_number in re_numbers:\n            number_len = 0\n            for i in re.finditer(re_number, text):\n                start = i.start() + number_len\n                end = i.end() + number_len\n                number: Any = text[start:end]\n                number = re.sub(\",\", \".\", re.sub(r\"\\.\", \"\", number.strip(\" ,.\")))\n                if number == \"\":\n                    continue\n                if self.is_float(number) or self.is_integer(number):\n                    try:\n                        if self.is_integer(number):\n                            number = int(number)\n                        else:\n                            number = float(number)\n                        number = num2words(number, to=\"cardinal\", lang=\"id\")\n                        text = text[:start] + number + text[end:]\n                        number_len += len(number) - (end - start)\n                    except NotImplementedError as error:\n                        print(error)\n                        print(f\"Problem with number: &lt;{text}&gt;: {number}\")\n        return text\n\n    def normalize(self, text: str) -&gt; str:\n        \"\"\"Normalizes Indonesian text by expanding:\n\n        - URL\n        - Currency\n        - Measurements\n        - Dates\n        - Timezones\n        - Arabic Numerals\n\n        Args:\n            text (str): Text to normalize.\n\n        Returns:\n            str: Normalized text.\n        \"\"\"\n        # Remove URL\n        text = self.normalize_url(text)\n        # Currency\n        text = self.normalize_currency(text)\n        # Measurements\n        text = self.normalize_measurement(text)\n        # Date\n        text = self.normalize_date(text)\n        # Timezones\n        text = self.normalize_timezone(text)\n        # Any number\n        text = self.normalize_number(text)\n        # collapse consecutive whitespaces\n        text = re.sub(r\"\\s+\", \" \", text)\n        return text\n</code></pre>"},{"location":"reference/textprocessor/#g2p_id.text_processor.TextProcessor.is_float","title":"<code>is_float(number)</code>  <code>staticmethod</code>","text":"<p>Check if float by type-casting.</p> <p>Parameters:</p> Name Type Description Default <code>number</code> <code>Any</code> <p>Number to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Is a valid float.</p> Source code in <code>g2p_id/text_processor.py</code> <pre><code>@staticmethod\ndef is_float(number: Any) -&gt; bool:\n    \"\"\"Check if float by type-casting.\n\n    Args:\n        number (Any): Number to check.\n\n    Returns:\n        bool: Is a valid float.\n    \"\"\"\n    try:\n        float(number)\n        return True\n    except ValueError:\n        return False\n</code></pre>"},{"location":"reference/textprocessor/#g2p_id.text_processor.TextProcessor.is_integer","title":"<code>is_integer(number)</code>  <code>staticmethod</code>","text":"<p>Check if integer by type-casting.</p> <p>Parameters:</p> Name Type Description Default <code>number</code> <code>Any</code> <p>Number to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Is a valid integer.</p> Source code in <code>g2p_id/text_processor.py</code> <pre><code>@staticmethod\ndef is_integer(number: Any) -&gt; bool:\n    \"\"\"Check if integer by type-casting.\n\n    Args:\n        number (Any): Number to check.\n\n    Returns:\n        bool: Is a valid integer.\n    \"\"\"\n    try:\n        int(number)\n        return True\n    except ValueError:\n        return False\n</code></pre>"},{"location":"reference/textprocessor/#g2p_id.text_processor.TextProcessor.normalize","title":"<code>normalize(self, text)</code>","text":"<p>Normalizes Indonesian text by expanding:</p> <ul> <li>URL</li> <li>Currency</li> <li>Measurements</li> <li>Dates</li> <li>Timezones</li> <li>Arabic Numerals</li> </ul> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to normalize.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Normalized text.</p> Source code in <code>g2p_id/text_processor.py</code> <pre><code>def normalize(self, text: str) -&gt; str:\n    \"\"\"Normalizes Indonesian text by expanding:\n\n    - URL\n    - Currency\n    - Measurements\n    - Dates\n    - Timezones\n    - Arabic Numerals\n\n    Args:\n        text (str): Text to normalize.\n\n    Returns:\n        str: Normalized text.\n    \"\"\"\n    # Remove URL\n    text = self.normalize_url(text)\n    # Currency\n    text = self.normalize_currency(text)\n    # Measurements\n    text = self.normalize_measurement(text)\n    # Date\n    text = self.normalize_date(text)\n    # Timezones\n    text = self.normalize_timezone(text)\n    # Any number\n    text = self.normalize_number(text)\n    # collapse consecutive whitespaces\n    text = re.sub(r\"\\s+\", \" \", text)\n    return text\n</code></pre>"},{"location":"reference/textprocessor/#g2p_id.text_processor.TextProcessor.normalize_currency","title":"<code>normalize_currency(self, text)</code>","text":"<p>Normalizes international and Indonesian (Rupiah) currencies.</p> <p>Examples:</p> <ul> <li><code>\"$250\"</code> -&gt; <code>\"dua ratus lima puluh dollar\"</code></li> <li><code>\"Rp 3,000,000\"</code> -&gt; <code>\"tiga juta rupiah\"</code></li> </ul> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text with currency to normalize.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Normalized text with currency transliterated.</p> Source code in <code>g2p_id/text_processor.py</code> <pre><code>def normalize_currency(self, text: str) -&gt; str:\n    \"\"\"Normalizes international and Indonesian (Rupiah) currencies.\n\n    Examples:\n    - `\"$250\"` -&gt; `\"dua ratus lima puluh dollar\"`\n    - `\"Rp 3,000,000\"` -&gt; `\"tiga juta rupiah\"`\n\n    Args:\n        text (str): Text with currency to normalize.\n\n    Returns:\n        str: Normalized text with currency transliterated.\n    \"\"\"\n    moneys = re.findall(self.re_moneys, text)\n    for money in moneys:\n        number: Any = re.sub(\",\", \".\", re.sub(r\"\\.\", \"\", money[2].strip(\" ,.\")))\n        try:\n            if number == \"\":\n                continue\n            if self.is_integer(number):\n                number = int(number)\n            elif self.is_float(number):\n                number = float(number)\n            else:\n                number = re.sub(r\"[.,]\", \"\", number)\n                number = int(number)\n            number = num2words(number, to=\"cardinal\", lang=\"id\")\n            text = text.replace(\n                money[0].strip(\" ,.\"),\n                f\"{number} {money[3]} {self.currencies[money[1]]}\",\n            )\n        except NotImplementedError as error:\n            print(error)\n            print(f\"Problem with money: &lt;{text}&gt;: {number}\")\n    return text\n</code></pre>"},{"location":"reference/textprocessor/#g2p_id.text_processor.TextProcessor.normalize_date","title":"<code>normalize_date(self, text)</code>","text":"<p>Normalizes dates.</p> <p>Examples:</p> <ul> <li><code>\"(12/3/2021)\"</code> -&gt; <code>\"dua belas Maret dua ribu dua puluh satu\"</code></li> </ul> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text with dates to normalize.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Normalized text with dates transliterated.</p> Source code in <code>g2p_id/text_processor.py</code> <pre><code>def normalize_date(self, text: str) -&gt; str:\n    \"\"\"Normalizes dates.\n\n    Examples:\n    - `\"(12/3/2021)\"` -&gt; `\"dua belas Maret dua ribu dua puluh satu\"`\n\n    Args:\n        text (str): Text with dates to normalize.\n\n    Returns:\n        str: Normalized text with dates transliterated.\n    \"\"\"\n    dates = re.findall(r\"(\\((\\d{1,2})/(\\d{1,2})(/(\\d+))?\\))\", text)\n    for date in dates:\n        try:\n            day = num2words(int(date[1]), to=\"cardinal\", lang=\"id\")\n            month: Any = int(date[2]) - 1\n            if month &gt;= 12:\n                month = 0\n            month = self.months[month]\n            if date[4] != \"\":\n                year = num2words(int(date[4]), to=\"cardinal\", lang=\"id\")\n                date_string = f\"{day} {month} {year}\"\n            else:\n                date_string = f\"{day} {month}\"\n            text = text.replace(date[0], f\" {date_string} \")\n        except NotImplementedError as error:\n            print(error)\n            print(f\"Problem with dates: &lt;{text}&gt;: {date}\")\n    return text\n</code></pre>"},{"location":"reference/textprocessor/#g2p_id.text_processor.TextProcessor.normalize_measurement","title":"<code>normalize_measurement(self, text)</code>","text":"<p>Normalizes measurement units, including its scalar value.</p> <p>Examples:</p> <ul> <li><code>\"10,5 km\"</code> -&gt; <code>\"sepuluh koma lima kilometer\"</code></li> <li><code>\"5\u00b0C\"</code> -&gt; <code>\"lima derajat celsius\"</code></li> </ul> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text with measurements to normalize.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Normalized text with measurements transliterated.</p> Source code in <code>g2p_id/text_processor.py</code> <pre><code>def normalize_measurement(self, text: str) -&gt; str:\n    \"\"\"Normalizes measurement units, including its scalar value.\n\n    Examples:\n    - `\"10,5 km\"` -&gt; `\"sepuluh koma lima kilometer\"`\n    - `\"5\u00b0C\"` -&gt; `\"lima derajat celsius\"`\n\n    Args:\n        text (str): Text with measurements to normalize.\n\n    Returns:\n        str: Normalized text with measurements transliterated.\n    \"\"\"\n    units = re.findall(self.re_measurements, text)\n    for unit in units:\n        number: Any = re.sub(\",\", \".\", re.sub(r\"\\.\", \"\", unit[1].strip(\" ,.\")))\n        try:\n            if number == \"\":\n                continue\n            if re.search(r\"\\.\", number):\n                number = float(number)\n            else:\n                number = int(number)\n            number = num2words(number, to=\"cardinal\", lang=\"id\")\n            text = text.replace(\n                unit[0].strip(\" ,.\"), f\"{number} {self.measurements[unit[2]]}\"\n            )\n        except NotImplementedError as error:\n            print(error)\n            print(f\"Problem with measurements: &lt;{text}&gt;: {number}\")\n    return text\n</code></pre>"},{"location":"reference/textprocessor/#g2p_id.text_processor.TextProcessor.normalize_number","title":"<code>normalize_number(self, text)</code>","text":"<p>Normalizes Arabic numbers to Indonesian.</p> <p>Examples:</p> <ul> <li><code>\"1.000\"</code> -&gt; <code>\"seribu\"</code></li> <li><code>\"10,5\"</code> -&gt; <code>\"sepuluh koma lima\"</code></li> </ul> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text with numbers to normalize.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Normalized text with numbers transliterated.</p> Source code in <code>g2p_id/text_processor.py</code> <pre><code>def normalize_number(self, text: str) -&gt; str:\n    \"\"\"Normalizes Arabic numbers to Indonesian.\n\n    Examples:\n    - `\"1.000\"` -&gt; `\"seribu\"`\n    - `\"10,5\"` -&gt; `\"sepuluh koma lima\"`\n\n    Args:\n        text (str): Text with numbers to normalize.\n\n    Returns:\n        str: Normalized text with numbers transliterated.\n    \"\"\"\n    re_numbers = [r\"([\\d.,]+)\", r\"\\d+\"]\n    for re_number in re_numbers:\n        number_len = 0\n        for i in re.finditer(re_number, text):\n            start = i.start() + number_len\n            end = i.end() + number_len\n            number: Any = text[start:end]\n            number = re.sub(\",\", \".\", re.sub(r\"\\.\", \"\", number.strip(\" ,.\")))\n            if number == \"\":\n                continue\n            if self.is_float(number) or self.is_integer(number):\n                try:\n                    if self.is_integer(number):\n                        number = int(number)\n                    else:\n                        number = float(number)\n                    number = num2words(number, to=\"cardinal\", lang=\"id\")\n                    text = text[:start] + number + text[end:]\n                    number_len += len(number) - (end - start)\n                except NotImplementedError as error:\n                    print(error)\n                    print(f\"Problem with number: &lt;{text}&gt;: {number}\")\n    return text\n</code></pre>"},{"location":"reference/textprocessor/#g2p_id.text_processor.TextProcessor.normalize_timezone","title":"<code>normalize_timezone(self, text)</code>","text":"<p>Normalizes Indonesian time with timezones.</p> <p>Examples:</p> <ul> <li><code>\"22.30 WITA\"</code>     -&gt; <code>\"dua puluh dua lewat tiga puluh menit Waktu Indonesia Tengah\"</code></li> </ul> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text with timezones to normalize.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Normalized text with timezones transliterated.</p> Source code in <code>g2p_id/text_processor.py</code> <pre><code>def normalize_timezone(self, text: str) -&gt; str:\n    \"\"\"Normalizes Indonesian time with timezones.\n\n    Examples:\n    - `\"22.30 WITA\"`\n        -&gt; `\"dua puluh dua lewat tiga puluh menit Waktu Indonesia Tengah\"`\n\n    Args:\n        text (str): Text with timezones to normalize.\n\n    Returns:\n        str: Normalized text with timezones transliterated.\n    \"\"\"\n    timezones = re.findall(self.re_timezones, text)\n    for timezone in timezones:\n        try:\n            hour = num2words(int(timezone[1]), to=\"cardinal\", lang=\"id\")\n            minute = num2words(int(timezone[2]), to=\"cardinal\", lang=\"id\")\n            zone = self.timezones[timezone[3]]\n            if minute == \"nol\":\n                time_string = f\"{hour} {zone}\"\n            else:\n                time_string = f\"{hour} lewat {minute} menit {zone}\"\n            text = text.replace(timezone[0], f\"{time_string}\")\n        except NotImplementedError as error:\n            print(error)\n            print(f\"Problem with timezones: &lt;{text}&gt;: {timezone}\")\n    return text\n</code></pre>"},{"location":"reference/textprocessor/#g2p_id.text_processor.TextProcessor.normalize_url","title":"<code>normalize_url(self, text)</code>","text":"<p>Removes URL from text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text with URL to normalize.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Normalized text with URLs removed.</p> Source code in <code>g2p_id/text_processor.py</code> <pre><code>def normalize_url(self, text: str) -&gt; str:\n    \"\"\"Removes URL from text.\n\n    Args:\n        text (str): Text with URL to normalize.\n\n    Returns:\n        str: Normalized text with URLs removed.\n    \"\"\"\n    urls = re.findall(self.re_http, text)\n    for url in urls:\n        text = text.replace(url[0], \"\")\n    return text\n</code></pre>"}]}